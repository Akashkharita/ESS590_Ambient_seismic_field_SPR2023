{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from ambient seismic field\n",
    "\n",
    "\n",
    "This tutorial will give examples on how to extract features from ambient field data. We will ue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# seismic packages\n",
    "import obspy\n",
    "from obspy.clients.fdsn.client import Client \n",
    "from obspy.geodetics.base import gps2dist_azimuth\n",
    "import obspy.signal\n",
    "client=Client(\"IRIS\")\n",
    "\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rc('font', size=12) \n",
    "matplotlib.rc('font', size=12) \n",
    "\n",
    "# scipy for seismic data processing\n",
    "import scipy as sc\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "# feature extraction packages\n",
    "import tsfel\n",
    "import seis_feature     # this is a compilation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download noise data using obspy. Let's pick an interesting station for noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = obspy.UTCDateTime(\"2022-12-20\")\n",
    "tr=client.get_waveforms(network=\"UW\",station=\"HSR\",channel=\"HHZ\",location=\"*\",starttime=t0,endtime=t0+86400,attach_response=True);\n",
    "# tr.remove_response()\n",
    "tr.plot() \n",
    "fs=tr[0].stats.sampling_rate\n",
    "print(fs)\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks quite noisy with no apparent change in the amplitudes.  We will proceed with increasing complexity in the features.  \n",
    "## 1. Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft, fftfreq, next_fast_len\n",
    "\n",
    "npts = tr[0].stats.npts\n",
    "## FFT the signals\n",
    "# fill up until 2^N value to speed up the FFT\n",
    "Nfft = next_fast_len(int(tr[0].data.shape[0])) # this will be an even number\n",
    "freqVec = fftfreq(Nfft, d=tr[0].stats.delta)[:Nfft//2]\n",
    "tr.taper(max_percentage=0.05)\n",
    "Zhat = fft(tr[0].data,n=Nfft)#/np.sqrt(Z[0].stats.npts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(11,8))\n",
    "ax.plot(freqVec,np.abs(Zhat[:Nfft//2])/Nfft)\n",
    "ax.grid(True)\n",
    "ax.set_xscale('log');ax.set_yscale('log')\n",
    "ax.set_xlabel('Frequency (Hz)');ax.set_ylabel('Velocity spectrum (m/s)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Sxx = signal.spectrogram(tr[0].data, fs = tr[0].stats.sampling_rate,nperseg=40*256)\n",
    "plt.pcolormesh(t/3600, f,np.log10(Sxx),vmin=-2,vmax=3,cmap=\"turbo\")\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [hours]')\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.1,50])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Features used in event detections\n",
    "\n",
    "1. STA/LTA to detect earthquakes (at almost all seismic networks)\n",
    "2. Kurtosis to detect earthquakes (at several seismic networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.trigger import classic_sta_lta, plot_trigger, recursive_sta_lta\n",
    "\n",
    "# short time scale:\n",
    "sta = 5 # seconds\n",
    "lta = 100 # seconds\n",
    "cft = recursive_sta_lta(tr[0].data, int(sta * fs), int(lta * fs))\n",
    "plot_trigger(tr[0], cft, 1.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.realtime.signal import kurtosis\n",
    "z=kurtosis(tr[0])\n",
    "plt.plot(z)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract features using tsfel\n",
    "\n",
    "TSFEL is an open-source package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = tsfel.get_features_by_domain()\n",
    "df = tsfel.time_series_features_extractor(cfg, tr[0].data[:100*100], fs= tr[0].stats.sampling_rate, window_size=len(tr[0].data[:100*100])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will sweep through the data with a 10-min window length and add features to the data frame. We will reshape the data into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = tr[0].stats.sampling_rate\n",
    "wlen = int(10 * 60 * fs)\n",
    "step =  wlen\n",
    "nwindow = int(np.floor(len(tr[0].data)/wlen))\n",
    "nmax = int(wlen*np.floor(len(tr[0].data)/wlen))\n",
    "print(nwindow,wlen,nmax)\n",
    "\n",
    "data = np.reshape(tr[0].data[:nmax],(nwindow,wlen))\n",
    "plt.plot(data)\n",
    "# plt.pcolormesh(data,vmin=-0.002,vmax=0.001)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will sweep through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = tsfel.get_features_by_domain()\n",
    "df=[]\n",
    "for i in range(nwindow):\n",
    "    if df is None:\n",
    "        df=tsfel.time_series_features_extractor(cfg, data[i,:], fs= fs, window_size=wlen)\n",
    "    else:\n",
    "        df.append(tsfel.time_series_features_extractor(cfg, data[i,:], fs= fs, window_size=wlen))\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate other features that have been useful in volcano seismology. We will follow Manuela's work on how to calculate RSAM/DSAR\n",
    "# Workflow RSAM & DSAR\n",
    "\n",
    "**The first step is to calculate the data stream. There are several sub-steps:**\n",
    "1. After removing the instrument response to the seismic signals, apply a bandpass filter to each 24 hours of data, between 2-4.5, 4-8 and 8-16 Hz (corresponding to the RSAM, MF and HF bands).\n",
    "2. Compute the absolute values of each signal.\n",
    "3. Subdivide the signals into 10 minutes intervals. For each interval, compute the average value as the RSAM, MF and HF datapoints assigned to that interval.\n",
    "4. Removing outliers associated with regional earthquakes is optional. We procced as follow: from (2), subdivide the signals into 10 minutes intervals. \n",
    "    1. Calculated the mean and standard deviation (mu and sigma) for each interval. \n",
    "    2. Apply z-score normalization in log-space to the interval using mu and sigma. \n",
    "    3. Check if any value in the interval exceeds a threshold of 3.2 standard deviations above the mean. \n",
    "    4. If yes, exclude data points from a 150s mask starting 15s before the outlier located. \n",
    "    5. Calculate the average value in the interval excluding points inside the mask: this the RSAM, MF and HF value for the interval.\n",
    "**To calculate the DSAR, procced as follow:**\n",
    "1. Integrate the bandpass filtered MF and HF data with time.\n",
    "2. Take the absolute value and compute averages on 10-minute intervals.\n",
    "3. Compute the ratio between integrated MF and HF.\n",
    "**For computing the DSAR median feature proposed as a precursor in this paper follow:**\n",
    "1. Every 10 minutes in the DSAR data stream, take a 48 hours window (looking backwards).\n",
    "2. For each window, compute the median.\n",
    "3. Construct the feature time series with point every 10 minutes corresponding the medians computed from the 48 hours windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.detrend('demean')\n",
    "tr.taper(0.05, type='hann')\n",
    "tr.merge(fill_value=0)\n",
    "pre_filt = [1e-3, 5e-2, 45, 50]\n",
    "water_level = 60\n",
    "tr[0].remove_response(zero_mean=True,taper=True, taper_fraction=0.05,\n",
    "                            pre_filt=pre_filt, output=\"VEL\", water_level=water_level,\n",
    "                            plot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(datas, ti, freqs_names, df):\n",
    "    datas = np.array(datas)\n",
    "    time = [(ti+j*600).datetime for j in range(datas.shape[1])]\n",
    "    df_tr = pd.DataFrame(zip(*datas), columns=freqs_names, index=pd.Series(time))\n",
    "    df = pd.concat([df, df_tr])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [[2, 5], [4.5, 8], [8,16]]\n",
    "freqs_names = ['rsam','mf','hf', 'dsar', 'ndsar']\n",
    "datas=[]\n",
    "datas = seis_feature.RSAM(tr[0], fs, datas, freqs[0], nmax, wlen) # get RSAM for different frequency bands\n",
    "\n",
    "datas, dsar = seis_feature.DSAR(data, fs, datas, freqs_names, freqs, nmax,wlen)\n",
    "datas = seis_feature.nDSAR(datas, dsar)\n",
    "df = create_df(datas, tr[0].stats.starttime, freqs_names, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, sharex=True, figsize=(6.4*2, 4.8))\n",
    "# ax[0].plot(st_long[0].times('matplotlib'), st_long[0].data, color='k', \n",
    "#            label='{}.{}..{}'.format(st_long[0].stats['network'],st_long[0].stats['station'],st_long[0].stats['channel']))\n",
    "ax.plot(df['rsam'], label='RSAM')\n",
    "ax.plot(df['mf'], label='MF')\n",
    "ax.plot(df['hf'], label='HF')\n",
    "ax.plot(np.nan, label='DSAR')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(df['dsar'], label='DSAR', color='C3')\n",
    "ax2.set_ylim(0,6)\n",
    "\n",
    "ax3 = ax.twinx()\n",
    "ax3.plot(df['ndsar'], label='nDSAR', color='C4')\n",
    "\n",
    "#ax.legend(ncol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
